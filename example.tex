\documentclass[10pt, conference]{IEEEtran}
\usepackage{subimages}
\setfigdir{figs}
\usepackage[cmex10]{amsmath}
\interdisplaylinepenalty=2500

\usepackage{amsthm}
\newtheorem{definition}{Definition}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{hyperref}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{todonotes}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{balance}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Real Time Eye Gaze Tracking Using Microsoft Kinect}

%------------------------------------------------------------------------- 
% change the % on next lines to produce the final camera-ready version 
\newif\iffinal
\finalfalse
% \finaltrue
\newcommand{\jemsid}{61}
%------------------------------------------------------------------------- 

% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

\iffinal
  \author{%
    \IEEEauthorblockN{Raul Benites Paradeda}
    \IEEEauthorblockA{%
      INESC-ID \& Instituto Superior T\'{e}cnico\\
      University of Lisbon\\
      Lisbon, Portugal\\
      Email: \href{mailto:raul.paradeda@tecnico.ulisboa.pt}{raul.paradeda@tecnico.ulisboa.pt}}
  \and
    \IEEEauthorblockN{Jones Granatyr, Jean Paul Barddal}
    \IEEEauthorblockA{%
      Graduate Program in Informatics (PPGIa)\\
      Pontif\'{i}cia Universidade Cat\'{o}lica do Paran\'{a} (PUCPR)\\
      Curitiba, Brazil\\
      Email:  \href{mailto:jones.granatyr@pucpr.edu.br}{jones.granatyr@pucpr.edu.br}, \href{mailto:jean.barddal@ppgia.pucpr.br}{jean.barddal@ppgia.pucpr.br}}
   \and
    \IEEEauthorblockN{Alberto Signoretti}
    \IEEEauthorblockA{%
      Graduate Program in Computer Science \\
      State Universtity of Rio Grande do Norte (UERN)\\
      Natal, Brazil\\
      Email:  \href{mailto:albertosignoretti@uern.br}{albertosignoretti@uern.br}}
  }
\else
  \author{Sibgrapi paper ID: \jemsid \\ }
\fi

%------------------------------------------------------------------------- 
% Special Sibgrapi teaser
\teaser{%
  \oneimage{From the left, in the first image we can see the lines generated by the gaze estimation. The next figure shows the pre-defined area that has an individual timer. The third image shows the area of interest created by our method, while the next one presents the rectangle created around the eye. Finally, it is possible to observe in the last image the result obtained by our method using the clustering algorithm.}{.95}{pic1.png}
}
%------------------------------------------------------------------------- 

% make the title area
\maketitle

\begin{abstract}
This work presents a technique used to identify in real time, the focus region of the user's gaze through of a Kinect device, and how long the userâ€™s focus is maintained over a specific region of the environment. The technique is divided in two stages. In the first one, the capture of the gaze is performed in two parts, the first one uses predefined regions, and the second is based on regions created using as criteria the user focus of the length of time on each part of the scenario. The second stage performs an algorithm of classification to identify the iris position. As a result, the technique showed that is possible identify and measure how long time the user is gazing to a region and if it is predefined or not. Besides, the log of data related the user's eye were correctly captured and K-means algorithm was performed with success, with real possibilities of allow the correct identification of the iris position that will be performed in a future work.
\end{abstract}

\begin{IEEEkeywords}
	Gaze tracking; Area of interest; Iris tracking.
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle

\section{Introduction}

	Since the beginning of the humankind, vision has became one of the most important senses to stay alive in a harsh environment. 
	In that time, vision was one of the most important sense to survive, create tools, hunt, and adapt to different environments. 
	According to \cite{1} capabilities such as sight became a crucial advantage to raise the probabilities of survival in the first ages.
	Sight is so important that, even the eyes have communicative value well before the development of the spoken language \cite{2}.
	This assertion is so true that, after the language system has been mastered and made commonplace, eyes are still useful in communication. 
	Eyes send obvious signals with specific messages, such as eye-rolling or subtle winks; or they can be more subtle, such as controlling eye contact to show or withhold intimacy \cite{3}.

	Nowadays, eyes are not only used to create tools or hunt, but mainly as a communication tool in social interaction \cite{4}.
	During communication, one generally does not rely on voice alone, but also on other non-verbal cues, such as body movement, hand movement, gazing, among others.
	Gaze cues are claimed to be used from early life and continue to be given and followed throughout adulthood \cite{4}.
	People have a tendency to orient and follow gaze cues of others, and it can be done with relative ease.
	This effect occurs in many situations, ranging from simple aids to achieve some object, until the attempt to read another player in a poker game, for instance.

	The influence of the gaze that a person has over another is so important that several studies about this subject can be found in the literature.
	For example, how gaze cues result in gaze or action following by the observer? 
	The work of \cite{5} describes four studies in which the authors investigated the infant's ability to connect gaze and emotional expression to intentional action.
	In short, this work argues that gazing is a crucial way in which infants recognize the intentional behaviors of other people. 
	The authors obtained this result by repeating an experiment with 8-month, 12-month and 14-month-old children. 
	The experiment aimed to observe that an adult gaze before grasp one of two objects, thereby giving cues. 
	Then, as result they found that at the end of the first year of life, humans can better predict other's actions with the presence of gaze or emotional expression.
    
	Another work is the one by \cite{6}, where authors argue that social conflict is a form of relating and that gaze clues are critical to understanding the underlying cognitive processes in this phenomenon. 
	They created an experimental setting that reduces real life to a mixed-motive game, and analyzed 22 gaze patterns from 10- to 12-year-old children in specific game moments that could have been conducted to conflict.
	Authors concluded that children gazes show that they are being more competitive or cooperative at different stages of the game.
	Children avoid confrontation by averting face-directed gazes when asking for larger profits, while longer gazes indicate the attempt to persuade others.

	The present work is a proposal of a tool to automate the evaluation performed during the experiment described in \cite{6}.   
	The conclusion of these authors was made manually, i.e. experts watched videos of the children's games play aiming to determine how long each children spent gazing to another children.
	To improve this method, we developed a module that is capable of capturing an user's gaze and automatically measures the time of each one of them.
	Using angles provided by Microsoft Kinect and vector formulas, we captured user's gaze and its duration.
	Additionally, our tool also captured and read information about users' eyes regions and created a dataset composed of color and coordinates information.
	Afterwards, it was necessary to apply a clustering algorithm to identify regions and similarities. In this way, we intend to identify which pixels has similar patterns and which can identify the iris, with this, the position in the screen.
    
	This paper is organized as follows.
	In Section \ref{sec:relatedWork} we present related work that uses Microsoft Kinect to identify user's gaze.
	Later, Section \ref{sec:technologyUsed} it is briefly described the two versions of Microsoft Kinect, to support our choice.
	Section \ref{sec:eyeGazeEstimation} describes our proposal for eye gaze identification and length measuring, while Section \ref{sec:dataExtraction} discusses about data extraction.
	Finally, Section \ref{sec:conclusionAndFutureWork} concludes this paper and puts forward future work.

\section{Related Work} \label{sec:relatedWork}

	In the work of \cite{7}, authors proposed a system that adopts a personalized 3D face model along with an eye detection gaze through iris, using the first version of Microsoft Kinect. 
	To identify the iris they used the Starburst algorithm \cite{8} with some modifications. 
	This experiment was performed starting with the calibration phase, where the participants had to look to nine predefined points in the screen, and afterwards, several images were taken of the participants face for each one of the points observed. 
	Then, to the phase of gaze estimation, participants were asked to gaze to a predefined point. 
	After one session, each participant was asked to move his/her seat position before starting the next step. 
	They performed this method several times aiming to collect the data that was used to calibrate the system.
	According to the results, authors concluded that Microsoft Kinect provides more accurate gaze estimation compared with RGB images extracted by conventional cameras. Another important conclusion was that their calibration mode reduced the degree of errors.

	In other work, authors in \cite{8} used a system to calibrate the iris position using the nine points method \cite{9}, which randomizes some points in the screen and asks the user to gaze each point, one by one. 
	Their tests embedded two scenarios: the first one consisted of separating the gaze motion in local motion, while the second one aimed at separating it in global motion. 
	According to them, the local motion refers to the gaze motion driven by the pupil movement, while the global motion is related to motion driven by the head movements. 
	The authors compared their results with the work of \cite{9} and they proposed a method that performs exceptionally well for the reliability and precision in gaze identification.

	On the other hand, authors in \cite{10} proposed a method to estimate the gaze direction in a 3D space, using the first version of Microsoft Kinect. 
	They combined depth and visual data to create texture 3D mesh of the scene. 
	Besides, parameters of head-pose were used to stabilize the 3D scene and to generate eye images. 
	As result, they could estimate the gaze direction under challenging head poses and low-resolution eye images.

\section{Microsoft Kinect} \label{sec:technologyUsed}
    
	As previously mentioned, the Microsoft Kinect device was used to provide the necessary information for gaze identification.
	This section aims to discuss some important differences between the first and the second version of this device.
	This is particularly important because most of the related work employ the first version, and we argue that the new features available in the current version can improve the overall performance of the device.

	This device has an infrared projector, cameras and a special microchip to track the movement of objects and people in three dimensions. 
	Furthermore, it has a 3D depth sensor, tilt motor and microphones.
	The second version received some improvements when compared to the first version. 
	For example, the first version of Kinect has a color image resolution of 640 x 480 pixels with a field of view (fov) of 62 x 48.6 degrees, which results in an average of about 10 x 10 pixels per degree. 
	On the other hand, the second version presents color image resolution of 1920 x 1080 pixels and a fov of 84.1 x 53.8, resulting in an average of about 22 x 20 pixels per degree. 
	This improvements imply in better image quality, thus, possibly making better detection of data contained in retrieved images.

	Another important difference is in the depth image, where the Kinect v1 has a resolution of 320 x 240 pixels with a fov of 58.5 x 46.6 degrees, what results in an average of about 5 x 5 pixels per degree. 
	Conversely, the Kinect v2 has a depth image resolution of 512 x 424 pixels with a fov of 70.6 x 60 degrees, resulting in an average of about 7 x 7 pixels per degree. 

	According to Microsoft specifications for Kinect for Windows, the Kinect v2 face recognition, motion tracking, and resolution are much more precise than the Kinect v1. 
    Kinect v2 uses ``time of flight'' technology to determine the features and motion of certain objects. 
	Besides, the v2 can process 2 gigabytes of data per second. The adopted USB 3.0 provides almost ten times faster broadband for the data transfer, 60\% wider field of vision, and it can detect and track 20 joints from 6 peopleâ€™s bodies including thumbs. 
	Besides, when using Kinect v2 it is possible to detect heart rates, facial expressions and weights on limbs, along with much more extremely valuable biometric data, as one can see in Fig. \ref{fig:fig2}.

    \begin{figure}[t]
        \centering
        \includegraphics{figures/pic2.png}
        \caption{Kinect v2 detections.}
        \label{fig:fig2}
    \end{figure}

	Our experiments were executed using Microsoft Kinect v2 since it provides better image quality, reliability and precision regarding face positioning.

	Another important trait of our project is the integration of Microsoft Kinect with its respective Software Development Kit (SDK) to compute some important values to the developer, as well as the width and height (in pixels) of some object on the screen.
    These values are returned by the methods of \emph{KinectSensor} object, namely \emph{ColorFrameSource} and \emph{DepthFrameSource}.
	Therewith, it is possible to set some regions such as user's head, and retrieve the width, the height, and his/her angle.

	However, for the purpose of this work another class of the SDK Kinect is more important, named \emph{Kinect.Face}. 
	Using this class, it is possible to capture some interesting information, such as: (i) rotation orientation, face engagement, (ii) if the user has glasses on, (iii) if the left and right eyes are closed, (iv) whether the user is looking away, (v) if the mouth has moved, (vi) or if it is open. 
	These values can be seen in Fig. \ref{fig:fig3}, where it is possible to observe the information available in the screen provided by the \emph{Kinect.Face} method.

    \begin{figure}[t]
        \centering
        \includegraphics{figures/pic3.png}
        \caption{Information provided by \emph{Kinect.Face} method.}
        \label{fig:fig3}
    \end{figure}

\section{Eye-Gaze Estimation} \label{sec:eyeGazeEstimation}

	The rotation orientation is important to determine the angle vector and the tracing of a gaze line. 
	This orientation is informed by the \emph{Kinect.Face} method and its values represent the axis $X$, $Y$ and $Z$, related to the user face rotation. 
	However, the values returned by the method are in a system number called ''quaternions'', and the values refer to: pitch (rotation about the X-axis), yaw (rotation about the Y-axis), and roll (rotation about the Z-axis) (see Fig. \ref{fig:fig4}). 
	Quaternions are mathematical structures that combine complex numbers and vector concepts \cite{2,11} that can be named as hyper-complex numbers or fourth dimension vectors. 
	The quaternions formula is given by Equation \ref{eq:quaternion}, where $a$, $b$, $c$ and $d$ are real numbers and $i$, $j$ and $k$ are basis elements of $H$.
    
    \begin{equation}
		H = \{a+ bi + cj + dk ~ \vert ~ a, b, c, d \in \mathbb{R} \}
        \label{eq:quaternion}
	\end{equation}

\begin{figure}[t]
	\centering
	\includegraphics{figures/pic4.png}
    \caption{Quaternions returned by \emph{Kinect.Face} method and their vertices.}
    \label{fig:fig4}
\end{figure}

	Therefore, quaternions values are used to calculate the Euler angles. 
	The Euler angles are three angles introduced by Leonhard Euler to describe the orientation of a rigid body, which is a body in which the relative position of all its points is constant \cite{12}. 
	Nevertheless, in this project the Euler angle must be given in degrees and it allows the prediction of the target of an user's gaze.
	The conversion code that transform the Euler values to degrees can be seen in code snippet in Listing \ref{code1}, where the $y$, $w$, and $z$ variables represent the angles provided by Kinect sensor, and $yawD$, $pitchD$ and $rollD$ are variables that will receive the values representing the degrees after the conversion.
   
\lstset{language=C++,
             basicstyle=\footnotesize,
             numbers=left,
			 stepnumber=1,
			 firstnumber=1,
             captionpos=b,
             numberstyle=\footnotesize,
             frame=shadowbox,
             rulesepcolor=\color{black}}

\begin{lstlisting}[caption = Convert face rotation quaternion to Euler angles in degrees., label={code1}]
double yawD, pitchD, rollD;
pitchD = Math.Atan2(2*((y*z)+(w*x)),
		 (w*w)-(x*x)-(y*y)+(z*z))
         /Math.PI*180.0;
yawD = Math.Asin(2*((w*y)-(x*z)))
		/Math.PI*180.0;
rollD = Math.Atan2(2*((x*y)+(w*z)),
		(w*w)+(x*x)-(y*y)-(z*z))
        /Math.PI*180.0;
\end{lstlisting}

	After the conversion, it is possible to calculate the heading vector. 
	One of the objective of this work is to trace a line from each point located in user face, such as: one point in each eye, one in the nose, and one in each side of the mouth, as shown in Fig. \ref{fig:fig5}. 
	From these points, a line will be generated based on the angle of user head. 
	Therefore, to trace this line it is necessary two points: begin point and end point, both provided in the pixels format. 
	The begin point is determined by the points in user face, while the end point must be calculated from the angle of user head. 
	A normalization is necessary to calculate the end point and to inform the orientation of the angles, informed by the \emph{Kinect.Face} method. 
	The normalization was performed given Equations \ref{eq:normalizationX} and \ref{eq:normalizationY}, where $X$ is the pitch and $Y$, both provided by \emph{Kinect.Face}.
    \begin{equation}
    	normX = \frac{X}{\sqrt{X^2 + Y^2}}
		\label{eq:normalizationX}
	\end{equation}
    \begin{equation}
    	normY = \frac{Y}{\sqrt{X^2 + Y^2}}    
		\label{eq:normalizationY}
	\end{equation}

    \begin{figure}[t]
        \centering
        \includegraphics{figures/pic5.png}
        \caption{Predefined face points.}
        \label{fig:fig5}
    \end{figure}

	After the normalization, the angle was converted into a vector by applying the arc cosine formula and converting the angle from degress to radians, which was done to axis X and Y.
	Equation \ref{eq:pitchTransformation} depicts the transformation to the pitch value, where $\arccos$ is the inverse cosine function and $normX$ is the normalized pitch angle. The equation \ref{eq:pitchTransformation} depicts the transformation to the yaw value, where $\arccos$ is the inverse cosine function and $normY$ is the normalized yaw angle (\ref{eq:yawTransformation}).

	\begin{equation}
    	PNX = \arccos{(normX)} \times \frac{180}{\pi}
		\label{eq:pitchTransformation}
	\end{equation}
    
    \begin{equation}
    	PNY = \arccos{(normY)} \times \frac{180}{\pi}
		\label{eq:yawTransformation}
	\end{equation}

	Finally, after this process these values are used to define the end point of the line. Although, first it was necessary to set a bias value to correct the X axis of the final point. Equation \ref{eq:lengthline} depicts this bias value, where $PNY$ is the yaw angle in radians, the numbers 12 and 140 represent values predefined based in visual tests, where the final point in relation of X axis was in a better position, to our experiment is on the edge of the screen .
    
    \begin{equation}
    	bias = (PNY \times 12) \div 140
		\label{eq:lengthline}
	\end{equation}
    
      Then, the equation \ref{eq:finalpoint} depicts the point where it is located the final point of the line according with the yaw normalized angle, where $PNY$ is the yaw angle in radians, the $bias$ is a bias to correct the X axis, $posH$ is the value of the height of the screen, and the number 3 is a value which defines the length of the line.
    
    \begin{equation}
    	PF = Point(PNY \times bias, posH \times 3)
		\label{eq:finalpoint}
	\end{equation}
   
     
	In short, the begin point is provided by Kinect SDK through the method \emph{Kinect.Face} and the end point is calculated after a series of formulas and conversions starting from the quaternions. 
	In Fig. \ref{fig:fig6} it is possible to visualize the line generated by the angle of user head at each head move. 
	In the first image, one can see that the user tilted the head to the left and lines were created to predict where the user gaze is. 
	This effect happens in the other images, where the lines predicting user gaze were created according to the tilt of user head.

  \begin{figure}[t]
      \centering
      \includegraphics{figures/pic6a.png}\\
      \includegraphics{figures/pic6b.png}\\
      \includegraphics{figures/pic6c.png}\\
      \includegraphics{figures/pic6d.png}
      \caption{Line determined by the head angle and simulating the user gaze.}
      \label{fig:fig6}
  \end{figure}

\subsection{Predefined Areas}

	As mentioned earlier, this work is an extension of \cite{6}, and the authors had to measure manually the time that the participant gazed to predefined areas, because of this, was necessary to our system define the areas where the gaze time will be measure. 
	The experiment accomplished in Campos (\cite{6}) was a game with cards, so the region where the players gaze is under his head (on the table) and also to the other player, located in front of him/her. 
	Thus, we defined in our approach four areas on the bottom of the screen (simulating the cards positions) and one on middle of the screen (simulating the other player), as we can see in Fig. \ref{fig:fig7}.

  \begin{figure}[t]
      \centering
      \includegraphics{figures/pic7.png}
      \caption{Regions predefined simulating the cards and other player.}
      \label{fig:fig7}
  \end{figure}

	After determining the important areas, it is necessary to calculate the time where the user gaze is on each one of these areas. 
	Thus, we created a method that starts a timer for each region when the end point of the gaze line reaches some region. 
	The time for each region is stored in an array, in which each position represents an observed area. 
	The time that the user gaze in each region is represented as a text on the left corner inside of each region, as we can see in Fig. \ref{fig:fig8}. 
	When the experiment is over, the time for each region is outputted to text files for future studies.

    \begin{figure}[t]
        \centering
        \includegraphics{figures/pic8.png}
        \caption{Seconds observed in each region represented on the left corner of each region.}
        \label{fig:fig8}
    \end{figure}


\subsection{Non-predefined Areas}

	In the first step of this work we showed that it is possible to capture the time and the region where the user is gazing at. 
	However, in some cases the experiment does not have a predefined area (as mentioned in the previous section) and the system must be able to recognize any special area that has the user's attention. 
	Based on that, a second prototype was developed, where the system recognizes an area that was gazed by the user. 
	Afterwards, we built a rectangle and counted the seconds that the user is gazing to this area (Fig. \ref{fig:fig9}). 
	This area is considered important when the user gaze at least for 2 seconds. 
	When the user gaze to another point of interest and it last 2 seconds, another region is built. It is possible to change the amount of time before running the experiment in the source code, and this number of 2 seconds was chosen randomly and it should defined according to the application objectives.
	As can be seen in Fig. \ref{fig:fig9}, in the first image the user gazed to a non-specific position for at least 2 seconds, so the system identified that this region can be an area of interest and it created a square. 
	Then, in the other image, the user gazed to another position for at least 2 seconds, so the system created another area of interest. 

    \begin{figure}[t]
        \centering
        \includegraphics{figures/pic9a.png}\\
        \includegraphics{figures/pic9b.png}\\    
        \caption{Area of interest defined by the system.}
        \label{fig:fig9}
    \end{figure}

	However, this method uses the head position, which can be problematic.
	For instance, the user can turn the head to the left but your gaze is centered. 
	It happens because of the iris position, and for this reason, it is necessary a future study about how to capture the iris position. 
    In the next section we detail the tests performed to reach this goal.

\section{Data Extraction} \label{sec:dataExtraction}

	After the identification of the region where the user is gazing, it is necessary to develop a more accurate system, which aims to identify userâ€™s iris and predict with more precision the region where the gaze is. 
	To reach this goal, it was necessary to define the user eye region to collect the data provided by the Kinect device.
	In our prototype we captured data of only one eye, since capturing all data provided by Microsoft Kinect related to screen is impractical because the quantity of information.
	In this sense, the unnecessary information was reduced, i.e., only information regarding the eyes were captured.
    However, our approach can be easily extended by paralleling this same procedure for both eyes.

    \begin{figure}[t]
        \centering
        \includegraphics{figures/pic10.png}
        \caption{Eye region identified by the yellow rectangle.}
        \label{fig:fig10}
    \end{figure}

	Using the rectangle coordinates around the eye, it is possible to read a lot of information, e.g. the position of each pixel in the screen, the color of each pixel, depth of each pixel, values of luminosity and infrared. 
	However, only values that allow the system to identify user's iris inside the region were used in our experiments, such as: R (red), G (green), B (blue), A (alpha control color opacity), Hex (hexadecimal value color), POS (pixel position inside of pixel array available by Kinect), X and Y (X and Y coordinates of the pixel).
	These data were obtained from an array that is available by the Kinect device, which has 8.294.400 positions, independently of the resolution of the display. 
	It is defined by the multiplication of the Kinect camera resolution (1920 x 1080), and this result is multiplied by four (representing R, G, B and A values). 
	Thereby, each position represent a color or opacity value and each screen line is represented by 7.680 array positions, also not mattering the screen resolution. 
    In other words, the first 7.680 array positions represent colors and opacity of the first line of pixels of the screen (Fig. \ref{fig:fig11}), so this value is obtained by dividing the total of the array positions for the resolution of Kinect camera, which is 1080p. 
	As mentioned before, each pixel of the screen is represented by four positions of the array, for example, the 0, 1, 2 and 3 array positions represent the pixel located in position 0x0 (X and Y) in screen coordinates. Similarly, the array positions 4, 5, 6 and 7 represent the pixel in position 0x1 (X and Y) in screen coordinates, and so forth.
	This approach to color representation used in Kinect poses some difficulties, such as the size of the array is constant independently of the screen resolution. 
	The problem happens because of the position of determined pixels in a 1920 x 1080 screen resolution, so the set of the four indices in the array (that represent these pixels) will be different in other resolution. 
	For example, if we have the following positions: X = 10 and Y = 0 (first row tenth pixel) at a 1920 x 1080 resolution, it will read the values 40, 41, 42 and 43 of the array. 
    On the other hand, in a 800 x 600 resolution the values that will be read are: 96, 97, 98 and 99. Due to this fact, it is necessary to create a heuristic that allows the reading of the correct positions of the array, regardless of display resolution. Otherwise, the positions in the array that represent the user eye were incorrect. 

  \begin{figure}[t]
      \centering
      \includegraphics{figures/pic11.png}
      \caption{Representation of color array available by Kinect device.}
      \label{fig:fig11}
  \end{figure}

	Thereby, to capture the information inside the rectangle created around the user eye (Fig. \ref{fig:fig10}), it is necessary to identify which array index each border is represented inside of the array that represents the screen (Fig. \ref{fig:fig11}), regardless of display resolution. 
	This is necessary because the array available by Kinect, as earlier mentioned, has 8.294.400 positions, which represent the color of each pixel in the screen. With regard to our approach, we just need the positions that represent the user eye. 
	In this way, it is necessary to search the array looking for the index that represents each border of the rectangle around the user eye (Fig. \ref{fig:fig12}). 
	At a first moment, it is necessary to determine an adjustment variable named $\sigma$, which is calculated according to Equation \ref{eq:sigma}, where $aH$ is the height in pixels of the display resolution.
    \begin{equation}
    	\sigma = \frac{(1080 - aH)}{aH}
		\label{eq:sigma}
	\end{equation}
    
	After the computation of the sigma variable, one must identify in which axis (X or Y) each rectangle line is positioned, given Equations \ref{eq:pt1} through \ref{eq:pt4}, where $aH$ is height in pixels of the display resolution and $aW$ is its width; $fBPos.Top$ is most on the top position on axis Y; $fBPos.Right$ is most on the right position on axis X; $fBPos.Bottom$ is lower pixel in Y; and $fBPos.Left$ is the position to the left in X.
    
	\begin{equation}
		cFTop = \floor*{\frac{fBPos.Top \times aH}{1080}}
        \label{eq:pt1}
	\end{equation}
    
    \begin{equation}
		cFRight=\floor*{\frac{fBPos.Right \times aW}{1920}}
	\end{equation}
    
    \begin{equation}
		cFBot=\floor*{\frac{fBPos.Bottom \times aH}{1080}}
	\end{equation}
    
    \begin{equation}
		cFLeft=\floor*{\frac{fBPos.Left \times aW}{1920}}
		\label{eq:pt4}
	\end{equation}


    \begin{figure}[t]
        \centering
        \includegraphics{figures/pic12.png}
        \caption{Indicating the face box position.}
        \label{fig:fig12}
    \end{figure}

	After the retrieval of the coordinates, it is identified which index of array colors that each rectangle corner represents.
	To reach this goal it was necessary to execute the code in Listing \ref{code2}. 
	To find the determined point, for example, the point that represent the left corner on the top of the rectangle, was necessary multiple the value that define the border on the left ($cFLeft$) by the resolution of the screen (1920). 
	This result is divided by the width screen, and them, it is multiplied by 4 because we have four fields, such as: values of R, G, B and A. 
	Finally, it is necessary to sum this result with the result of the next operation, that is, multiply 7.680 value (the number of index in one row of the array) by the sum of the number that represent the top position ($cFTop$) with the same value, multiplied by $\sigma$.
    
\begin{lstlisting}[caption = Calculate the array index that represent each corner of the rectangle., label={code2}, float=*]
iPTop = Convert.ToInt64(Math.Truncate(((((int)cFLeft * 1920) / aW) * 4) +
        (7680 * ((int)cFTop + Math.Truncate((int)cFTop * sigma)))));
fPTop = Convert.ToInt64(Math.Truncate(((((int)cFRight * 1920) / aW) * 4) +
        (7680 * ((int)cFTop + Math.Truncate((int)cFTop * sigma)))));
iPBot = Convert.ToInt64(Math.Truncate(((((int)cFLeft * 1920) / aW) * 4) + 
         (7680 * ((int)cFBot + Math.Truncate((int)correctFCPBot * sigma)))));
fPBot = Convert.ToInt64(Math.Truncate(((((int)cFRight * 1920) / aW) * 4) + 
        (7680 * ((int)cFBot + Math.Truncate((int)cFBot * sigma)))));
\end{lstlisting}

	Finally, after computing the index in the array that represents the border of the rectangle around user's eye, all values inside of the rectangle were read and written in a text file. 

	To validate if the capture of the values were in a correct way, it is generated an image from the data of this text file. 
	In this way, one can see that the capture was done in a way that it is possible to identify the user's eye, as depicted in Fig. \ref{fig:fig13}.

    \begin{figure}[t]
      \centering
      \includegraphics{figures/pic13.png}
      \caption{Image created based on the data in the text file.}
      \label{fig:fig13}
    \end{figure}

\subsection{Clustering}

	With the text file created in the extraction data step, we used a clustering algorithm called k-means \cite{13}. 
	This algorithm use a simple and easy way to group a given data set through a $k$ number of clusters, where $k$ is defined a priori or by some optimization process.
	In practice, k-means defines $k$ centroids, which are initially located somewhere in the feature space.
    There are several strategies to define the first location of these centroids, and one of the most common ways is to place them equally distributed along the problem space.
	The next step is to take each point belonging to a given dataset and associate it to the nearest centroid. 
    When no point is pending, the first step is completed and an early clustering is done. 
	At this point, the $k$ centroids are recomputed as the barycenters of the clusters obtained in the previous space.
	After we have these $k$ new centroids, a new binding has to be done between the same data set points and the nearest new centroid. 
    This process is repeated until the centroids do not change in comparison to the previous step or given some other criterion.

    \begin{figure}[t]
        \centering
        \resizebox{.35\columnwidth}{!}{
            \includegraphics{figures/kmeans.png}
        }\\
        \caption{K-Means test results, from top to bottom: $k=2$, $k=3$, $k=4$, $k=5$, $k=10$ and $k=15$.}
        \label{fig:kmeans}
    \end{figure}

	Although it can be proved that the procedure will always terminate, the k-means algorithm does not necessarily find the most optimal configuration. 
	The algorithm is also significantly sensitive to the initial randomly selected cluster centers. 
	The k-means algorithm can be executed multiple times to reduce this effect.
	In short, this algorithm has a characteristic to try to find similarities between examples and then put a classification in these (cluster number). In our project, the data used was the values in a text file representing the data inside of the rectangle around the user eye. 
	The main idea is attempt to find the user iris based on the classification made by this clustering algorithm. 
	Therefore, we have made tests using the $k$ value equal to 2, 3, 4, 5, 10 and 15, and its results can be seen in Fig. \ref{fig:kmeans}.
	As we can see in Fig. \ref{fig:kmeans}, in all tests it was possible to segment the iris in at least one cluster.


\section{Conclusion and Future Work} \label{sec:conclusionAndFutureWork}
	Some clues (such as pointing, gaze and language) are important to make the observer do what is desired. 
	In some scenarios, some clues are more important than others, e.g. autism.
	Children with autism are known to have difficulties in sharing attention with others, while some researches \cite{14} show that a reasonable proportion of autistic children do not show difficulties in following another's head turn.
	Gaze is important to identify what one desires, mainly in cases where one is unable to talk or express through indications, e.g. cases of infants and people with disabilities.
	Still, in cases such as in campus work, where the gaze was used in a game context and identified that the children tend to avoid confrontation by averting face-directed gazes when they are asking for something. In this case, they gaze longer to attempt to persuade one another.

	Our proposal is relevant to aid experiments where there is the need to identify the user gaze in real time and its duration at each region, all given by its head angle.
	Moreover, it was possible to create regions of interests according to the length of gazes towards some region, regardless these were predefined or not.
	Nevertheless, we identified some problems with the use of this technique solely using head positions. 
	One of them is if the head position is slightly tilted to a side, the user could be gazing to the contrary side or the center, and it can happens because of his/her iris position. 
	Because of that, in this first prototype an image was created from the data of each pixel of the eye region that was informed by the Microsoft Kinect device, showing that the data collected were correct. 

	Next, to improve the used technique, k-means clustering was performed to predict the user iris position. 
	This algorithm received as input the data which represent the user eye, and then the clusters were created according to the similarity between the pixels. 
	Based on that, it was possible to perceive that the user iris, depending of the number of clusters, can be identified.
	As future work, we intend to make the identification of the data that represent the user iris, using an automatic and real time fashion. 
	This would allow a more accurate prediction of user gaze independently of head angle.


\bibliographystyle{IEEEtran}
\balance
\bibliography{example}
\balance

\end{document}
